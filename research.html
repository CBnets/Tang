<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
  <head>
	 <meta http-equiv="content-type" content="text/html; charset=utf-8">
	 <meta name="featureription" content="your description goes here">
	 <meta name="keywords" content="your,keywords,goes,here">
	 <meta name="author" content="Your Name / Original design: Andreas Viklund - http://andreasviklund.com/">
	 <link rel="stylesheet" type="text/css" href="./css/variant-winter.css" media="screen,projection" title="Tang">
	 <title>Tang's Research</title>
  </head>

  <body>
    <div id="container">
		<div id="logo">
		  <h1><a href="index.html">Jinglei Tang</a></h1>
		  <h2> <a href="http://www.nwsuaf.edu.cn/">Northwest A&F University</a></h2> 
	   </div>

	   <div id="main">
		  <h2 class="hide">Site menu:</h2>
		  <ul id="navitab">
			 <li><a href="index.html">Home</a></li>
			 <li><a class="current" href="research.html">Research</a></li>
			 <li><a href="publications.html">Publications</a></li>
          <li><a href="readinglists.html">Reading Lists</a></li>
			 <li><a href="links.html">Links</a></li>
			 <li><a href="personal.html">Personal</a></li>
		  </ul>
        <!-- </div> -->

		  <div id="feature">
	       <!-- <h4>Contact</h4> -->
		    <!-- <p>  -->
          <!--   Australia Technology Park<br> -->
          <!--     13 Garden Street | Eveleigh, NSW 2015 | Australia<br> -->
          <!--       <b>Tel:</b> +61 2 9376 2223 | <b>Fax:</b> +61 2 9376 2027 <br> -->
          <!--         <b>Email:</b> jianlong dot zhou at nicta dot com dot au<br> -->
          <!--         </p> -->
			 <hr>
		  </div>
		 <ul> 
			<li><a href="#interactiveML">Computer Vision</a></li>
			<li><a href="#transparentML">Pattern Recognition</a></li>
			<li><a href="#bioVis">Image processing</a></li>
			<li><a href="#sar">Decision Support</a></li>
			<li><a href="#volumerendering">Machine learning</a></li>
		 </ul>
		
		
		  <!--<h2> <font color="#309633">Current Research</font> </h2> -->
		  
		  <!-- 
        <p>
          <ul>
            <li><h3>Interactive Machine Learning</h3></li> <a name="interactiveML"></a>
            <p>The overall goal of Interactive Machine Learning is to build an application-adaptive user 
              interface architecture for Machine Learning (ML) algorithms so that they can be used by domain 
              experts effectively in wider real-world applications. </p>
			  
            <li><h3>Transparent Machine Learning</h3></li><a name="transparentML"></a>

            <p>   
              <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
              
	           <a href="images\tml.png">
                <img src="./css/images/updateImages/tml.png" class="alignleft" height="150" width="200" alt="tml.png"></a>
                <h4>Transparent Machine Learning --- Revealing Internal States of Machine Learning </h4>
                <p>
                  This work concerns the revealing internal states of Machine Learning (ML) meaningfully so that users can understand what is going on inside ML and how to accomplish with the learning problem. As a result, ML process becomes more understandable and usable. It changes from a “black-box” to “transparent-box”.
                </p>
                <div id="feature">
                  <b>Selected Publication:</b><br>
                    Jianlong Zhou, Zhidong Li, Yang Wang and Fang Chen: Transparent Machine Learning – Revealing Internal States of Machine Learning. In IUI2013 Workshop on Interactive Machine Learning.
                    <a href="papers\iui2013_transparent_ml_camera.pdf">PDF</a>
                    <hr>
                  </div>
                  <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>

                </p>
          </ul>
        </p>

        <div style="margin:0;padding:0; height:3px;background-color:#6666CC;overflow:hidden"></div>
   -->


		  <!--<h2> <font color="#309633">Previous Research</font></h2> -->
		  
		  <!--
		  <p>
          <ul>
            <li><h3>Bioimage Informatics and Visualization</h3></li> <a name="bioVis"></a>
            <p>   
              <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
              
	           <a href="images\plos_cb.png">
                <img src="./css/images/updateImages/plos_cb.png" class="alignleft" height="150" width="150" alt="plos_cb.png"></a>
                <h4>Visualization and Analysis of 3D Microscopic Images </h4>
                <p>
                  In a wide range of biological studies, it is highly desirable to visualize and analyze 3D microscopic images. 
                  In this primer, we first introduce several major methods for visualizing typical 3D images and related multi-scale, 
                  multi-time-point, multi-color data sets. Then, we discuss three key categories of image analysis tasks, namely 
                  segmentation, registration, and annotation. We demonstrate how to pipeline these visualization and analysis 
                  modules using examples of profiling the single-cell gene-expression of C. elegans and constructing a map of 
                  stereotyped neurite tracts in a fruit fly brain.
                </p>
                <div id="feature">
                  <b>Selected Publication:</b><br>
                    Fuhui Long, Jianlong Zhou, and Hanchuan Peng. Visualization and Analysis of 3D Microscopic Images. Plos Computational Biology. 8(6), 2012.
                    <a href="http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1002519">PDF</a>
                    <hr>
                  </div>
                  <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>

                </p>
				
				<p>   
              <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
              
	           <a href="images\virtual_finger.jpg">
                <img src="./css/images/updateImages/virtual_finger.jpg" class="alignleft" height="150" width="150" alt="virtual_finger.jpg"></a>
                <h4>Virtual finger boosts three-dimensional imaging and microsurgery as well as terabyte volume image visualization and analysis </h4>
                <p>
                  3D bioimaging, visualization and data analysis are in strong need of 
				  powerful 3D exploration techniques. We develop virtual finger (VF) to generate 3D curves, 
				  points and regions-of-interest in the 3D space of a volumetric image with a single finger 
				  operation. VF enables instant 3D optical zoom-in imaging, 3D free-form optical 
				  microsurgery, and 3D visualization and annotation of terabytes of whole-brain image volumes. 
				  VF also leads to orders of magnitude better efficiency of automated 3D reconstruction of neurons. 
                </p>
                <div id="feature">
                  <b>Selected Publication:</b><br>
                    Hanchuan Peng, Jianyong Tang<b>+</b>, Hang Xiao<b>+</b>, Alessandro Bria<b>+</b>, <b>Jianlong Zhou+ (+ Equal contribution)</b>, Victoria Butler, Zhi Zhou, Paloma T. Gonzalez-Bellido, Seung W. Oh, Jichao Chen, Ananya Mitra, Richard Tsien, Hongkui Zeng, Giorgio Ascoli, Giulio Iannello, Michael Hawrylycz, Eugene Myers, and Fuhui Long. Virtual Finger Boosts Three-Dimensional Imaging and Microsurgery as well as Terabyte Volume Image Visualization and Analysis. Nature Communications, 5:4342, July 2014. (<a href="http://www.nature.com/ncomms/2014/140711/ncomms5342/pdf/ncomms5342.pdf">PDF</a>) <a href="#" onclick="stm(['BibTex','@article{zhou2014ncomms, <br/> &nbsp;&nbsp;&nbsp;  author    = {Peng, Hanchuan and Tang, Jianyong and Xiao, Hang and Bria, Alessandro and Zhou, Jianlong and Butler, Victoria and Zhou, Zhi and Gonzalez-Bellido, Paloma T. and Oh, Seung W. and Chen, Jichao and Mitra, Ananya and Tsien, Richard W. and Zeng, Hongkui and Ascoli, Giorgio A. and Iannello, Giulio and Hawrylycz, Michael and Myers, Eugene and Long, Fuhui}, <br/> &nbsp;&nbsp;&nbsp;   title     = {Virtual finger boosts three-dimensional imaging and microsurgery as well as terabyte volume image visualization and analysis}, <br/> &nbsp;&nbsp;&nbsp;   journal   = {Nature Communications}, <br/> &nbsp;&nbsp;&nbsp;  volume    = {5}, <br/> &nbsp;&nbsp;&nbsp; doi = {10.1038/ncomms5342}, <br/> &nbsp;&nbsp;&nbsp; ee = {http://www.nature.com/doifinder/10.1038/ncomms5342}, <br/> &nbsp;&nbsp;&nbsp;  year      = {2014}, <br/> &nbsp;&nbsp;&nbsp; month      = {July},  <br/> } '],Style[5])" onmouseout="htm()">(BibTex)</a>
                    <hr>
                  </div>
                  <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>

                </p>
				
-->

               <!--

                <li><h3>Spatial Augmented Reality</h3></li> <a name="sar"></a>
                <p>   

                  <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                  
	               <a href="images\WeldingSpotsAll.jpg">
                    <img src="./css/images/updateImages/WeldingSpotsAll.jpg" class="alignleft" height="150" width="150" alt="WeldingSpotsAll.jpg"></a>
                    <h4>Applying Spatial Augmented Reality to Facilitate In-Situ Support for Automotive Spot Welding Inspection </h4>
                    <p>
                      In automotive manufacturing, the quality of spot welding on car bodies needs to be inspected frequently. The manual inspection 
                      process suffers from inefficiencies and potential mistakes. This
                      research describes a Spatial Augmented Reality (SAR) based system that projects visual data onto arbitrary surfaces for providing
                      just-in-time information to a user in-situ within a physical work-cell. SAR facilitates presentation
                      of projected digital Augmented Reality (AR) information on surfaces of car bodies.  
                    </p>
                    <div id="feature">
                      <b>Selected Publication:</b><br>
                        Jianlong Zhou, Ivan Lee, Bruce Thomas, Roland Menassa, Anthony Farrant and Andrew Sansome. In-Situ Support for Automotive Manufacturing
                        Using Spatial Augmented Reality. International Journal of Virtual Reality. 11(1): 33-41, 2012.
                        <a href="http://wearables.unisa.edu.au/wp-content/plugins/bib2html/data///2011/Applying%20Spatial%20Augmented%20Reality%20to%20Facilitate/p195-zhou.pdf">PDF</a>
                        <hr>
                      </div>
                      <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>

                    </p>

                    <li><h3>Volume Rendering and Medical Image Analysis</h3></li> <a name="volumerendering"></a>
                    <p>
                      <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                      
	                   <a href="images\CT-Knee-AutoTFHatR1Clip.jpg">
                        <img src="./css/images/updateImages/CT-Knee-AutoTFHatR1Clip.jpg" class="alignleft" height="150" width="150" alt="CT-Knee-AutoTFHatR1Clip.jpg"></a>
                        <h4>Automatic transfer function generation using contour tree controlled residue flow model and color harmonics</h4>
                        <p>
                          This research presents an approach
                          for automating transfer function (TF) generations by utilizing topological attributes derived from the contour tree. 
                          The contour
                          tree acts as a visual index to segment volume, and captures associated topological attributes involved in volumetric data. A residue flow
                          model based on Darcy's Law is employed to control distributions of opacity between branches of the contour tree. Topological attributes
                          are also used to control color selection in a perceptual color space and create harmonic color TFs. 
                        </p>

                        <div id="feature">
                          <b>Selected Publication:</b><br>
                            Jianlong Zhou and Masahiro Takatsuka. Automatic transfer function generation using contour tree controlled residue flow model and color
                            harmonics. IEEE Transactions on Visualization and Computer Graphics, 15(6):1481–1488, November-December 2009.
                            <a href="papers\AutoTFGen.pdf">PDF</a>
                            <hr>
                          </div>
                        </p>
                        <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>

                        <p>   

                          <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                          
	                       <a href="images\PCPMakeCurrent.jpg">
                            <img src="./css/images/updateImages/PCPMakeCurrent.jpg" class="alignleft" height="250" width="150" alt="PCPMakeCurrent.jpg"></a>
                            <h4>A Concept of Volume Rendering Guided Search Process to Analyze Medical Data Set </h4>
                            <p>
                              This research presents an approach of parallel coordinates based parameter control panel (PCP). The PCP is used to control parameters of
                              focal region-based volume rendering (FRVR) during data analysis. 
                              Based on the PCP, a concept of volume rendering guided search process is proposed. The
                              search pipeline is divided into four phases. Different parameters of FRVR are recorded and modulated in the PCP during search phases. The
                              concept shows that volume visualization could play the role of guiding a search process in the rendition space to help users to
                              efficiently find local structures of interest.
                            </p>
                            <div id="feature">
                              <b>Selected Publication:</b><br>
                                Jianlong Zhou, Chun Xiao,  Zhiyan Wang, and Masahiro Takatsuka. A concept of volume rendering guided search process to analyze medical
                                data set. Computerized Medical Imaging and Graphics, Volume 32, Issue 2, pp. 140-149, March 2008.                 
                                <hr>
                              </div>
                              <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                            </p>

                            <p>
                              <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                              
	                           <a href="images\FootSilhouettesFocal.JPG">
                                <img src="./css/images/updateImages/FootSilhouettesFocal.JPG" class="alignleft" height="150" width="150" alt="FootSilhouettesFocal.JPG"></a>
                                <h4>Focal-Region Based Volume Rendering </h4>
                                <p>
                                  In this research, a new approach named focal region-based volume rendering for visualizing internal structures of volumetric data is
                                  presented. This approach presents volumetric information through integrating context information
                                  with a lens-like focal region rendering to show more detailed information. This feature-based approach contains three main
                                  components: 1) A feature extraction model to provide context
                                  information; 2) An efficient ray-bounded volume ray casting rendering to provide the detailed information in
                                  the focal region; 3) The tools used to manipulate focal regions. 
                                </p>
                                <div id="feature">
                                  <b>Selected Publication:</b><br>
                                    Jianlong Zhou, Zhiyan Wang and Klaus D. Tönnies. Focal region-based volume rendering. International Journal of Pattern Recognition and
                                    Artificial Intelligence, Vol. 20, No.5, 665-677, 2006.              
                                    <hr>
                                  </div>
                                  
                                  <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                                </p>

                                <p> 
                                  <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                                  
	                               <a href="images\HeadDistanceTF7.JPG">
                                    <img src="./css/images/updateImages/HeadDistanceTF7.JPG" class="alignleft" height="150" width="150" alt="HeadDistanceTF7.JPG"></a>
                                    <h4>Control of Object Visibility in Volume rendering  --- A Distance-Based Approach  </h4>
                                    <p>
                                      The essence of volume rendering can be regarded as a mechanism to
                                      determine visibility of redundant information and structures of interest using different approaches. This research introduces the
                                      distance which is defined by the user into volume rendering pipeline to control the visibility of structures. The distance based
                                      approach, which is named as distance transfer function, has the flexibility of transfer functions for depicting data information and the
                                      advantages of volume clippings for visualizing inner structures. The results show that the distance based approach is a powerful tool for
                                      volume data information depiction.
                                    </p>
                                    
                                    <div>
                                      <div id="feature">
                                        <b>Selected Publication:</b><br>
                                          Jianlong Zhou, Andreas Döring, and Klaus D. Tönnies. Control of object visibility in volume rendering --- a distance based approach.
                                          International Journal of Image and Graphics, Vol. 5, No. 4, pp.699-714, 2005.  
                                          <hr>
                                        </div>


                                        <hr>
                                      </div>
									  
									  
                                      <div style="margin:0;padding:0; height:2px;background-color:#339933;overflow:hidden"></div>
                                    </p>
-->

                                  </ul>
                                  
                                </p>
	                           </div>
							   
	

	                           <div id="sidebar">
		                          <h2>Latest News</h2>
								  <p>
								  No info..
								  </p>
								  <!--添加最近动态
                                <p>
                                  <a href="http://nips.cc/">NIPS 2013</a>: Submission deadline is on 31 May 2013 .<br>
                                <marquee direction="up" height="40" onmouseout="start()" onmouseover="stop()" scrollamount="2">
                                  <font color="#309633">
                                    <script language="JavaScript"> 
                                      　　 var timedate= new Date("31 May 2013"); 
                                      　　 var times= "NIPS 2013 Submission Deadline"; 
                                      　　 var now = new Date(); 
                                      　　 var date = timedate.getTime() - now.getTime(); 
                                      　　 var time = Math.floor(date / (1000 * 60 * 60 * 24)); 
                                      　　 if (time >= 0) 
                                      　　 document.write( " "+times+": "+time +" days left")
                                    </script>
                                  </font>
                                </marquee>
                              </p>
							  -->

		                        <h2>Links</h2>
			                     <ul>
				                    <li><a href="http://cie.nwsuaf.edu.cn/">College of Information Engineering</a></li>
									<li><a href="http://www.nwsuaf.edu.cn/">Northwest A&F University</a></li>
									<li><a href="http://sydney.edu.au/engineering/it/">School of Information Technologies</a></li>
									<li><a href="http://sydney.edu.au/">The University of Sydney</a></li>
			                     </ul>
	                         </div>
                          </div>

                          <div id="footer">
	                         <p>Copyright &copy; 2016 Jinglei Tang <!--&middot; Template design by <a href="http://andreasviklund.com/">Andreas Viklund</a>-->
							 <br>
                            </div>
                          </body>
                        </html>
